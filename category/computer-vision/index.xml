<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Houssem Ben Braiek</title>
    <link>https://hoss-bb.github.io/category/computer-vision/</link>
      <atom:link href="https://hoss-bb.github.io/category/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 05 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hoss-bb.github.io/media/icon_hu46dff4dbf79dc03e46b38f18d11d3be0_4210_512x512_fill_lanczos_center_3.png</url>
      <title>Computer Vision</title>
      <link>https://hoss-bb.github.io/category/computer-vision/</link>
    </image>
    
    <item>
      <title>From VGGNet to EfficientNet: Key Milestones in the Evolution of CNN Design</title>
      <link></link>
      <pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate>
      <guid></guid>
      <description>&lt;p&gt;Modular, Multipath, Factorized, Compressed, Scalable‚Ä¶ All in CNN Nowadays, but What Led us Here‚Ä¶We‚Äôll take you through the major milestones in the history of convolutional neural network design‚Ä¶Not only will you learn, but you‚Äôll have fun, too ü§ì&lt;/p&gt;
&lt;p&gt;Photo by &amp;ldquo;My Life Through A Lens&amp;rdquo; on Unsplash
Generally speaking, classification problems form the basis of computer vision models, which are used to solve more complex vision problems.&lt;/p&gt;
&lt;p&gt;The task of object detection, for example, involves detecting bounding boxes and classifying the objects within them. To fulfill a segmentation task, every pixel in the image must be classified.&lt;/p&gt;
&lt;p&gt;Therefore, convolutional neural networks (CNNs) were first used to solve the problem of image classification, and it is on this problem that researchers started competing. Through fast-tracked research on more accurate classifiers for ImageNet Challenge, they have addressed more general issues related to statistical learning of high-capacity neural networks, leading to significant advances in deep learning.&lt;/p&gt;
&lt;p&gt;‚ö†Ô∏è Important Notice: It is assumed the reader knows CNN fundamentals[1].&lt;/p&gt;
&lt;p&gt;VGGNet
We introduce our first CNN, named VGGNet [2]. It is the direct successor to AlexNet [3], which is credited with being the first ‚Äúdeep‚Äù neural network, and both networks have a common ancestor, which is Lecun‚Äôs LeNet [4].&lt;/p&gt;
&lt;p&gt;In this post, we start with VGGNet because of its particularity. Despite its age, this is one of the very few DL models that still hold up today, whereas most recent inventions are already outdated. Moreover, VGGNet establishes the common components and structures adopted by CNNs that followed.&lt;/p&gt;
&lt;p&gt;Figure 1: VGGNet Architecture
As you should know, a convolutional neural network starts with an input layer, as can be seen in Figure 1. It has the same dimensions as the input image, 224x224x3.&lt;/p&gt;
&lt;p&gt;Then, VGGNet stacks a first convolutional layer (CL 1) that includes 64filters of size 3x3 with active padding and strides of 1, which gives an output tensor with 224x224x64.&lt;/p&gt;
&lt;p&gt;Next, it stacks a CL 2 that uses the same filter size of 3x3 on 64 channels with the same configuration, resulting in a feature map of the same dimensions.&lt;/p&gt;
&lt;p&gt;Afterward, max pooling with a filter size of 2x2, active padding, and strides of 2, is used to reduce the spatial resolution of the feature maps, from 224x224x64 to 112x112x64.&lt;/p&gt;
&lt;p&gt;Note that max-pooling does not affect the feature map depth, and therefore the number of channels remains 64.&lt;/p&gt;
&lt;p&gt;Furthermore, I put &amp;ldquo;module 1&amp;rdquo; above these three layers because a module is defined as a processing unit operating at a certain resolution. Thus, VGGNet&amp;rsquo;s module 1 operates at 224x224 resolution and yields a feature map with a resolution of 112x112 on which module 2 continues to operate.&lt;/p&gt;
&lt;p&gt;Similarly, module 2 also has two CLs with 3x3 filters that are used to extract higher-level features, followed by max pooling to divide by 2 the spatial resolution, but the number of filters is multiplied by 2 to double the number of channels at the output feature map.&lt;/p&gt;
&lt;p&gt;No doubt you can already guess what will happen in the next modules. Each module processes the input feature map, doubling its channels, dividing by 2 its spatial resolution, and so on. It is not possible to do that forever; the spatial resolution is already 7x7 in module 6.&lt;/p&gt;
&lt;p&gt;Therefore, VGGNet includes a flatten operation to go from 3D to 1D, then applies one or two dense layers, and BAM!, a softmax is applied to compute the class scores (Here, it is 1000labels).&lt;/p&gt;
&lt;p&gt;All of this is stuff you‚Äôve seen in your CNN courses. Let us summarize the design patterns that VGGNet introduces to outperform all its predecessors in terms of accuracy:&lt;/p&gt;
&lt;p&gt;‚úÖ Modular Architecture allows symmetry and homogeneity within convolutional layers. Indeed, building blocks of convolutional layers with similar characteristics, then performing downsampling between the modules help to preserve valuable information at the feature extraction stage and enable the use of small filters. In other words, the field of perception of two 3x3 successive filters can be equivalent to that of a single 5x5. In addition, a cascade of small filters enhances the nonlinearity and can lead to even better accuracy than one layer with a larger filter. Besides, small filters speed up the computations on Nvidia GPUs.&lt;/p&gt;
&lt;p&gt;‚úÖ Max-pooling operation is an effective down-sampling method compared to average-pooling or a stridden convolution (with strides greater than 1). Essentially, max-pooling allows capturing invariances in data with spatial information. It is important to emphasize that this spatial information reduction is required for image classification tasks to reach the output of class scores, but also it is justified by the ‚Äúmanifold hypothesis‚Äù. In computer vision, the manifold assumption states that the realistic images in the 224x224x3 dimension space represent a very limited subspace.&lt;/p&gt;
&lt;p&gt;‚úÖ Pyramid Shape refers to an overall smooth downsampling combined with an increase in the number of channels throughout the architecture. Indeed, the multiplication of channels compensates for the loss of representational expressiveness caused by the continuous decrease of the spatial resolution of the learned feature maps. Throughout the layers, the feature space becomes synchronously narrower and deeper until it gets ready to be flattened and fed as an input vector to the fully-connected layers. Intuitively, each feature can be seen as an object whose presence will be quantified throughout the inference computation. Early convolutional layers capture basic shapes, so fewer objects are needed. Later layers combine these shapes to create more complex objects with multiple combinations, requiring a large number of channels to preserve them all.&lt;/p&gt;
&lt;p&gt;Inception
Next, let‚Äôs introduce the second CNN, which appeared in the same year but a little later than VGGNet [2], and whose name is Inception [5]. The name is inspired by Christopher Nolan‚Äôs famous movie, and this network sparked the debate about ‚Äúgoing for deeper CNNs,‚Äù which quickly turned into a problem. Indeed, deep learning researchers realized that the more they manage to correctly train a deeper neural network, the better the accuracy they achieve, especially when it comes to complex classification tasks like ImageNet. In a nutshell, more stacked layers boost the learning capacity of a neural network, enabling it to capture sophisticated patterns and generalize over complex sets of entries.&lt;/p&gt;
&lt;p&gt;Wait a minute‚òùÔ∏èI mean it when I said, ‚Äúwhen they manage to train a deeper network.‚Äù Stacking more layers has a cost indeed and makes it more difficult to train the neural networks.&lt;/p&gt;
&lt;p&gt;This is due to the gradient vanishing problem, which happens when the loss gradient is back-propagated through numerous computation layers, and gradually converges to tiny values, almost zeros. Hence, it becomes too complicated to train earlier layers of the network and this is a serious problem since these layers perform the feature extraction and transmit the distilled information to subsequent layers.&lt;/p&gt;
&lt;p&gt;To get a quick idea of ‚Äã‚Äãthe reasons, the back-propagation of the loss gradient uses the derivative chain rule to leverage previously-calculated gradients. In other words, the computation of a gradient linked to the first layers includes a term that is a product of gradients obtained from the subsequent layers. As the input data is normalized and the parameters are initialized between [0, 1], the gradients estimated in a neural network will be less than 1 too. Therefore, the intensity of the gradient signal gradually diminishes over time because of successive multiplications by a coefficient below 1.&lt;/p&gt;
&lt;p&gt;Let‚Äôs return to Inception, where researchers simulate several layers at a single depth level. In this way, the neural network‚Äôs learning capacity is enhanced, and its parameter space is expanded without going deeper to avoid vanishing gradients.&lt;/p&gt;
&lt;p&gt;Figure 2: Inception Block
Figure 2 is an inside look üîç at this novel multiscale processing layer. Focusing on the blue üü¶ components, we see an input layer of nxnx3 and an output layer of nxnxŒ£ki. Instead of applying k convolutional filters of size 3x3, multiple processing layers are applied in parallel. The same input is going to be passed through a 1x1 convolution, a 3x3 convolution, a 5x5 convolution, and a max pooling (with strides of 1 to maintain resolution) at the same time. Then, all the resulting feature maps of sizes: nxnxk1, nxnxk3, nxnxk5, and nxnxk are concatenated into an output feature map of size: nxnxŒ£ki.&lt;/p&gt;
&lt;p&gt;That‚Äôs it, this is an inception multi-scale layer that does not worküòÖ Here we are gonna see the red üü• components in action. The Inception layer is characterized by the aggregation of multi-scale features resulting from different fields of perception and processing paths. However, each path yields at least k channels where k is the number of input channels. Stacking several multi-scale layers would certainly raise Out Of Memory Exceptions.&lt;/p&gt;
&lt;p&gt;üîîRemember what I mentioned earlier: a downsampling stage using the max-pooling layer does not affect the number of channels.&lt;/p&gt;
&lt;p&gt;To overcome this, inception designers introduced pointwise convolutions, which are just classic convolution layers with only filters of resolution: 1x1, and less number of filters, r &amp;lt; k, thereby reducing the depth of feature maps efficiently without sacrificing the relevant data extracted at this level of processing.&lt;/p&gt;
&lt;p&gt;Inception introduced the following main design outcomes:&lt;/p&gt;
&lt;p&gt;‚úÖ Proliferate Paths is based on the idea of including a multiplicity of branches in the architecture to simulate ensembles of subnetworks within a single neural network. It is interesting to wonder what history can teach us. In the end, this kind of multi-scale layering is really heavy and has never been widely adopted, however, the ability to combine multiple paths within a layer sparked the development of the following neural network.&lt;/p&gt;
&lt;p&gt;‚úÖ Point-wise convolution is now an extremely useful and prevalent tool in computer vision. It is a cost-effective operation with a small parameter footprint, and its processing time is relatively fast. Using it, we can effectively reduce the number of channels we have in the output feature map, making the neural network require less memory and computing power. I know that it is quite paradoxical when you are confronted with it the first time üòµ and if you are still wondering ü§î how adding extra pointwise convolution layers reduces the number of parameters, keep in mind that the number of parameters of each layer is strongly influenced by the number of input channels.&lt;/p&gt;
&lt;p&gt;ResNet
Next up is ResNet [6], one of the most revolutionary deep learning inventions and one of the most cited research papersü•á. This popularity is due to the fact that ResNet is the first CNN that succeeds in stacking more than 100 layers. Back at that time, 100 layers were completely insane because of this gradient vanishing problem. Now, we are talking about a thousand billion parameters in transformers. When I tell you that ResNet has 100 layers, you should conclude that they solved the gradient vanishing problem for good, and that‚Äôs what made it famous. Today, we no longer hear about the gradient vanishing problem.&lt;/p&gt;
&lt;p&gt;Let‚Äôs examine the revolutionary trick together. Once again, we maintain the VGGNet modular architecture, and we change the content of a single block. As shown in Figure 3, a residual block has nxnx3 as input and also nxnx3 as output. The channel count preservation is a MUST in such blocks.&lt;/p&gt;
&lt;p&gt;Figure 3: Residual Block
üîç Let us take a look inside. As usual, one of the processing paths will be a standard convolution with 3x3 filters, then, next to it, we add a shortcut/skip connection üí°, i.e., an identity function that passes the input directly to the output. That‚Äôs all, folks; this is one of the most revolutionary innovations in deep learning. It might seem odd at first, but the more you think about it, the more logical it becomes.&lt;/p&gt;
&lt;p&gt;Therefore, a Resnet will be a stack of modules plus downsampling, then further modules plus downsampling again, etc. Within the modules, there will be residual blocks, which contain shortcut connections that link the input to the output. Hence, it is possible to imagine a kind of information highway that runs from the exit (output) to the entrance (input), passing exclusively through these shortcut connections. That is exactly why ResNet solves the problem of the gradient vanishing.&lt;/p&gt;
&lt;p&gt;Still don‚Äôt get it? üòµ Think again about how there will be no loss of gradient when the latter flows through the shortcut connection (an identity function). Once you get itüëç, you might still wonder ü§î why does it preserve high learning capacity and even enhance it?&lt;/p&gt;
&lt;p&gt;Intuitively, two things need to be considered:&lt;/p&gt;
&lt;p&gt;A) First, ResNet‚Äôs shortcuts allow for bypassing any unnecessary processing levels given input data because some of the layers in a deep neural network can be related to detecting certain patterns that apply to a subset of objects.&lt;/p&gt;
&lt;p&gt;B) Second, we can assume that the input contains the response and the goal of computational layers is to refine it until the class is deduced. Hence, it makes sense to maintain the original input flow while adding the processing outcomes at every level as an iterative refinement.&lt;/p&gt;
&lt;p&gt;üì¢ From ResNet, the ImageNet community decided not to do the challenge every year since the problem is solved and the debate is almost over. As a result, researchers have become more interested in finding more efficient ways to solve the problem. For instance, they try to reduce the total number of FLOPS or memory footprints. That‚Äôs what I‚Äôm going to discuss in the following neural networks.&lt;/p&gt;
&lt;p&gt;MobileNet
Our first CNN in the era of cost minimization is MobileNet [7]. It is a compact CNN with fewer parameters, which runs fast on mobile platforms while delivering high performance.&lt;/p&gt;
&lt;p&gt;üí° The trick of MobileNet lies in factorizing the convolution operation into a two-stage superefficient processing. It‚Äôs time to get down to details.&lt;/p&gt;
&lt;p&gt;Figure 4
As can be seen in Figure 4, a classic convolutional layer passes 3D filters, where each one yields a channel in the output feature map. As an alternative, MobileNet proposes a depthwise convolution, where a bunch of only 2D filters is applied separately, i.e., each filter passes over all the channels of the input tensor.&lt;/p&gt;
&lt;p&gt;Here, I imagine you tell me ü§î all they did is the application of a classic convolution, except that they took the results of a single filter, and instead of summing them up, they kept them all non-concatenated.&lt;/p&gt;
&lt;p&gt;I agree with youü§ù doing this, we will end up with a lot of separate feature maps, and there will no longer be any correlation between the different channels if we just concatenate them. Hence, we miss two points: firstly, the feature maps need to be linked, and secondly, the channels need to be changed.&lt;/p&gt;
&lt;p&gt;üîîRemember what I said in Inception: when we want to change the number of channels at a low cost, we use a point-wise convolution.&lt;/p&gt;
&lt;p&gt;Thus, MobileNet indeed applies point-wise convolution to the feature maps that came out of the depthwise convolution. As can be seen in Figure 5, we can use as many 1x1 filters as we want to generate as many channels as we want. For instance, we do times k to get our typical feature map with the size of nxnxk.&lt;/p&gt;
&lt;p&gt;Figure 5
üîç If we look inside the MobileNet module, well here in Figure 5, we will see first a depth-wise convolution that operates on the 2D spatial information, then a point-wise convolution that merges and processes the channels‚Äô information along the z-dimension.&lt;/p&gt;
&lt;p&gt;MobileNet v2
It‚Äôs not the end. MobileNet has released a second version. MobileNet v2 [8] is a residual neural network, i.e., it stacked residual blocks to go deeper, except that within the layers, it breaks up the convolution operation to be cost-effective as well.&lt;/p&gt;
&lt;p&gt;üí°MobileNet v2 splits the layers into a group that handles high dimensional data processing and another that compresses and transmits the information to adjacent layers.&lt;/p&gt;
&lt;p&gt;Figure 6: Inverted Residual Block
üîç let us take a look at the MobileNet v2 block in Figure 6. First, we use a pointwise convolution to reach high dimensions k &amp;gt; r, then, we analyze the information effectively using depthwise convolution, and afterward, we return to low dimensions using, once again, pointwise convolution. Furthermore, we emphasize that a residual shortcut connection should be added from the input to the output layer.&lt;/p&gt;
&lt;p&gt;You may wonder ü§î how such compression can work without degrading performance. If that is the case, I invite you to think again of the manifold assumption behind the use of systematic downsampling since VGGNet.&lt;/p&gt;
&lt;p&gt;EfficientNet
You have made it to EfficientNet [9] which is the last CNN I‚Äôm going to discuss in this post. Despite it being released at the end of 2019, it is getting old. This network is going off the charts and outperforms by far all the other neural networks, as can be seen in Figure 7.&lt;/p&gt;
&lt;p&gt;Figure 7: Model Size vs. Accuracy Comparison [ref]
Now, let us see in detail what makes it so powerful. EfficientNet is MobileNetv2 with a twist on network sizing. Honestly, I hope you will not be disappointed üòí by the twist, as it is really simple but very effective.&lt;/p&gt;
&lt;p&gt;Hence, EffectiveNet stacks up inverted residual blocks as well but questions the arbitrary choice of depth, width, and resolution of the neural network. In a nutshell, the depth of the network corresponds to the number of layers in a network. The width is associated with the number of neurons in a layer or, more pertinently, the number of filters in a convolutional layer. The resolution is the height and width of the input.&lt;/p&gt;
&lt;p&gt;üí°EfficientNet designers propose a simple, albeit effective scaling technique that uses a compound coefficient …∏ to uniformly scale network width, depth, and resolution in a principled way. …∏ is a user-defined, global scaling factor (integer) that controls how many resources are available, whereas Œ±, Œ≤, and Œ≥ determine how to assign these resources to network depth, width, and resolution, respectively.&lt;/p&gt;
&lt;p&gt;As a result, the hyperparameters ‚Äî Œ±, Œ≤, and Œ≥- can be determined using grid search by setting …∏=1. Due to the small network size, the computation would be fast. When the optimal hyperparameters are determined, the compound coefficient …∏ can be increased to get larger and more accurate models. This is how different versions of EfficientNet: B1 to B7 are constructed, with the integer next to B indicating the value of the compound coefficient.&lt;/p&gt;
&lt;p&gt;üëä Quite surprisingly, using this clever network sizing heuristic üß™ outperforms all the state-of-the-art CNNs, even though all the design structures and patterns are the same as MobileNet v2.&lt;/p&gt;
&lt;p&gt;Photo by Brina Blum on Unsplash
Exhausted ü•± We are done. We made a trip back in time together üïï, since 2014 is the prehistory of deep learning. Now you can recognize a VGGNet, ResNet, or EfficientNet when you see them. Furthermore, you are aware of the significant improvements that were gradually implemented in CNN and what problems they solved.&lt;/p&gt;
&lt;p&gt;Nevertheless, I have only scratched the surface of these papers. It is worth reading the experiments that are presented in many of them to learn the journey of research that has been made to achieve the contribution.&lt;/p&gt;
&lt;p&gt;In summary, don‚Äôt hesitate to find out more, and in particular, don‚Äôt hesitate to check out EfficientNetv2 [10], which was released in late 2021.&lt;/p&gt;
&lt;p&gt;References
[1] Sumit Saha, A Comprehensive Guide to Convolutional Neural Networks, TDS, 2018&lt;/p&gt;
&lt;p&gt;[2] Simonyan et al., Very Deep Convolutional Networks for Large-Scale Image Recognition, 2014&lt;/p&gt;
&lt;p&gt;[3] Krizhevsky et al., ImageNet Classification with Deep Convolutional Neural Networks, 2012&lt;/p&gt;
&lt;p&gt;[4] Yann Lecun, LeNet-5 convolutional neural networks, 1998&lt;/p&gt;
&lt;p&gt;[5] Szegedy et al., Going Deeper with Convolutions, 2014&lt;/p&gt;
&lt;p&gt;[6] He et al., Deep Residual Learning for Image Recognition, 2016&lt;/p&gt;
&lt;p&gt;[7] G. Howard et al., MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, 2017&lt;/p&gt;
&lt;p&gt;[8] Sandler et al., MobileNetV2: Inverted Residuals and Linear Bottlenecks, 2018&lt;/p&gt;
&lt;p&gt;[9] Tan et al., EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, 2019&lt;/p&gt;
&lt;p&gt;[10] Tan et al., EfficientNetV2: Smaller Models and Faster Training, 2021&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
