<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Houssem Ben Braiek</title>
    <link>https://houssembenbraiek.me/event/</link>
      <atom:link href="https://houssembenbraiek.me/event/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 22 Aug 2024 15:30:00 +0000</lastBuildDate>
    <image>
      <url>https://houssembenbraiek.me/media/icon_hu46dff4dbf79dc03e46b38f18d11d3be0_4210_512x512_fill_lanczos_center_3.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://houssembenbraiek.me/event/</link>
    </image>
    
    <item>
      <title>Ensuring the Reliability, Robustness, and Ethical Compliance of LLMs</title>
      <link>https://houssembenbraiek.me/talk/ensuring-the-reliability-robustness-and-ethical-compliance-of-llms/</link>
      <pubDate>Thu, 22 Aug 2024 15:30:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/ensuring-the-reliability-robustness-and-ethical-compliance-of-llms/</guid>
      <description>&lt;p&gt;Large Language Models (LLMs) have revolutionized natural language processing by enabling human-like text generation and conversation. However, their non-deterministic nature, coupled with the complexities of modern applications, raises significant concerns around reliability, robustness, and ethical compliance. This presentation post delves into the challenges faced when integrating LLMs into real-world systems, particularly around testing and validation. We explore the limitations of traditional unit testing in handling LLM-generated outputs, offering new approaches such as LLM judges, black-box confidence estimation, and post-run output validation. Furthermore, we introduce tools like DeepEval and Guardrails AI, which help testing and guarding LLMs&amp;rsquo; generations while ensuring alignment with business, legal, and ethical standards. The discussion highlights the importance of balancing creative, generative power with rigorous validation processes to maintain trustworthiness in LLM applications. Through this comprehensive examination, we aim to provide practical solutions for improving the reliability, robustness and ethical governance of LLM systems in production environments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Domain-Aware DL Model Testing</title>
      <link>https://houssembenbraiek.me/talk/introduction-to-domain-aware-dl-model-testing/</link>
      <pubDate>Wed, 24 Jan 2024 09:30:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/introduction-to-domain-aware-dl-model-testing/</guid>
      <description>&lt;p&gt;As of today, engineers are building DL systems for high-risk, non-quantifiable performance scenarios such as autonomous cars. This talk highlights the pressing need to incorporate domain knowledge into more rigorous and deterministic testing methodologies for DL models to ensure that these black-box solutions encapsulate the intended behaviors (in line with prior beliefs) and respond as expected within their operational domain data. The oracle problem in DL systems requires more flexible assertion tests, like invariance and directional expectation, to verify the model&amp;rsquo;s behavior even without ground truth. The presentation introduces domain-aware deep learning (DL) model testing, which complements the usual statistical assessments by addressing the challenges posed by limited training samples and pipeline underspecification. It includes results of previous research on the implementation of invariance and directional expectation tests. Additionally, it introduces semantically-preserving data transformations and their utility for generating valid inputs from complex domains using existing data. Furthermore, generative AI breakthroughs will enable us to create these complex entries using statistical learning, just as software tests are written in code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Trustworthy Deep Learning Software System</title>
      <link>https://houssembenbraiek.me/talk/towards-trustworthy-deep-learning-software-system/</link>
      <pubDate>Fri, 17 Nov 2023 16:00:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/towards-trustworthy-deep-learning-software-system/</guid>
      <description>&lt;p&gt;This presentation addresses the trustworthiness of Deep Learning (DL) software by highlighting the unique challenges and distinctions between DL and traditional software. It delves into the complexities of debugging DL training programs, examining issues such as the &amp;ldquo;Untestable&amp;rdquo; nature of these programs and the practical implications of the Oracle Problem. The discussion includes property-based debugging of DL training programs, identifying common pitfalls, and detailing verification routines across pre-training, proper-fitting, and post-fitting stages. Additionally, the presentation discusses limitations in DL model testing, such as under-specification and the varied perceptions of testing value among practitioners. It emphasizes the necessity of domain-aware DL model testing methods, including invariance tests and directional expectation tests, and demonstrates their application in testing DL-powered aircraft performance models. Finally, it explores the potential of semantically-preserving data transformations and proposes that DL tests can be generated through statistical modeling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hand‑on Guide on Testing Deep Neural Networks</title>
      <link>https://houssembenbraiek.me/talk/a-handon-guide-on-testing-deep-neural-networks/</link>
      <pubDate>Wed, 18 May 2022 14:30:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/a-handon-guide-on-testing-deep-neural-networks/</guid>
      <description>&lt;p&gt;Data-driven AI (e.g., deep learning) has become a driving force and has been applied in many applications across diverse domains. The human-competitive performance makes them stand as core components in complicated software systems for tasks, e.g., computer vision (CV) and natural language processing (NLP). Corresponding to the increasing popularity of deploying more powerful and complicated DL models, there is also a pressing need to ensure the quality and reliability of these AI systems. However, the data-driven paradigm and black-box nature make such AI software fundamentally different from classical software. To this end, new software quality assurance techniques for AI-driven systems are thus challenging and needed. In this tutorial, we introduce the recent progress in AI Quality Assurance, especially for testing techniques for DNNs and provide hands-on experience. We will first give the details and discuss the difference between testing for traditional software and AI software. Then, we will provide hands-on tutorials on testing techniques for feed-forward neural networks (FNNs) with a CV use case and recurrent neural networks (RNNs) with an NLP use case. Finally, we will discuss with the audience the success and failures in achieving the full potential of testing AI software as well as possible improvements and research directions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Domain-Aware Deep Learning Testing for Aircraft System Models</title>
      <link>https://houssembenbraiek.me/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/</link>
      <pubDate>Wed, 18 May 2022 14:30:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/</guid>
      <description>&lt;p&gt;With deep learning (DL) modeling, aircraft performance system simulators can be derived statistically without the extensive system knowledge required by physics-based modeling. Yet, DL&amp;rsquo;s rapid and economical simulations face serious trustworthiness challenges. Due to the high cost of aircraft flight tests, it is difficult to preserve held-out test datasets that cover the full range of the flight envelope for estimating the prediction errors of the learned DL model. Considering this risk of test data underrepresentation, even low-error models encounter credibility concerns when simulating and analyzing system behavior under all foreseeable operating conditions. Therefore, domain-aware DL testing methods become crucial to validate the properties and requirements for a DL-based system model beyond conventional performance testing. Crafting such testing methods needs an understanding of the simulated system design and a creative approach to incorporate the domain expertise without compromising the data-driven nature of DL and its acquired advantages.&lt;/p&gt;
&lt;p&gt;Therefore, we present our physics-guided adversarial testing method that leverages foreknown physics-grounded sensitivity rules associated with the system and metaheuristic search algorithms in order to expose regions of input space where the DL model violates the system properties distilled from the physics domain knowledge. We also demonstrate how the revealed adversarial inputs can be relevant for model regularization to penalize these physics inconsistencies and to improve the model reliability. Furthermore, the verification of DL model sensitivity at different data points against physics-grounded rules pose scalability challenges when it comes to sequential models that predict dynamical system behavior over successive time steps. In response to that, we encode the directional expected variation of the predicted quantities using a simple physics model that is stochastically calibrated on the experimental dataset. Thus, in addition to assessing the error rate of the sequential DL model on genuine flight tests, we examine the extent to which the trajectory of its time series predictions matches the expected dynamics of variation under craftly-designed synthetic flight scenarios.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Coding Session on Debugging Neural Networks Programs</title>
      <link>https://houssembenbraiek.me/talk/hands-on-coding-session-on-debugging-neural-networks-programs/</link>
      <pubDate>Tue, 03 Mar 2020 15:00:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/hands-on-coding-session-on-debugging-neural-networks-programs/</guid>
      <description>&lt;p&gt;The goal of this hands-on session is to practice troubleshooting neural network training programs. The provided example consists of a VGG-like ConvNet trained on CIFAR-10 classification. If we fix the injected bugs (that I know), it should achieve more than 80% accuracy on the test set after more than 20 epochs. The debugging process is more relevant than finding the bug. Hence, we will walk through the steps you should follow towards hunting bugs. It is not about doing an in-depth review of code based on the participant&amp;rsquo;s Tensorflow coding skills, but rather about codifying verification routines in order to monitor the dynamics of critical variables and components that may indicate coding errors, misconfigurations, or training anomalies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Coding Session on Metamorphic Testing of Neural Networks</title>
      <link>https://houssembenbraiek.me/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/</link>
      <pubDate>Thu, 23 May 2019 14:00:00 +0000</pubDate>
      <guid>https://houssembenbraiek.me/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/</guid>
      <description>&lt;p&gt;Automating large-scale test case generation requires using multiple image transformations to build a large number of metamorphic transformations and their follow-up tests, in order to find DNN&amp;rsquo;s erroneous behaviors. In fact, the metamorphic transformation should be designed so that the transformed and original inputs are semantically equivalent. Thus, the first coding task is to implement metamorphic transformations on the input, assembling all the provided image-based transformations to guarantee the diversity of generated inputs. In order to estimate the level of logic explored by inputs, neuronal coverage criteria are employed, which estimate the amount of activation states covered by inputs. It is therefore essential that we store each valid synthetic input that fools the DNN under test. Hence, the second coding task is to develop the follow-up test that takes the logits returned by the DNN for the transformed data to check if the test fails or succeeds, then stores the synthetic images corresponding to the failed tests.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
