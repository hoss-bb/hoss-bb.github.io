<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Houssem Ben Braiek</title>
    <link>https://hoss-bb.github.io/event/</link>
      <atom:link href="https://hoss-bb.github.io/event/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 18 May 2022 14:30:00 +0000</lastBuildDate>
    <image>
      <url>https://hoss-bb.github.io/media/icon_hu46dff4dbf79dc03e46b38f18d11d3be0_4210_512x512_fill_lanczos_center_3.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://hoss-bb.github.io/event/</link>
    </image>
    
    <item>
      <title>A Handâ€‘on Guide on Testing Deep Neural Networks</title>
      <link>https://hoss-bb.github.io/talk/a-handon-guide-on-testing-deep-neural-networks/</link>
      <pubDate>Wed, 18 May 2022 14:30:00 +0000</pubDate>
      <guid>https://hoss-bb.github.io/talk/a-handon-guide-on-testing-deep-neural-networks/</guid>
      <description>&lt;p&gt;Data-driven AI (e.g., deep learning) has become a driving force and has been applied in many applications across diverse domains. The human-competitive performance makes them stand as core components in complicated software systems for tasks, e.g., computer vision (CV) and natural language processing (NLP). Corresponding to the increasing popularity of deploying more powerful and complicated DL models, there is also a pressing need to ensure the quality and reliability of these AI systems. However, the data-driven paradigm and black-box nature make such AI software fundamentally different from classical software. To this end, new software quality assurance techniques for AI-driven systems are thus challenging and needed. In this tutorial, we introduce the recent progress in AI Quality Assurance, especially for testing techniques for DNNs and provide hands-on experience. We will first give the details and discuss the difference between testing for traditional software and AI software. Then, we will provide hands-on tutorials on testing techniques for feed-forward neural networks (FNNs) with a CV use case and recurrent neural networks (RNNs) with an NLP use case. Finally, we will discuss with the audience the success and failures in achieving the full potential of testing AI software as well as possible improvements and research directions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Domain-Aware Deep Learning Testing for Aircraft System Models</title>
      <link>https://hoss-bb.github.io/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/</link>
      <pubDate>Wed, 18 May 2022 14:30:00 +0000</pubDate>
      <guid>https://hoss-bb.github.io/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/</guid>
      <description>&lt;p&gt;With deep learning (DL) modeling, aircraft performance system simulators can be derived statistically without the extensive system knowledge required by physics-based modeling. Yet, DL&amp;rsquo;s rapid and economical simulations face serious trustworthiness challenges. Due to the high cost of aircraft flight tests, it is difficult to preserve held-out test datasets that cover the full range of the flight envelope for estimating the prediction errors of the learned DL model. Considering this risk of test data underrepresentation, even low-error models encounter credibility concerns when simulating and analyzing system behavior under all foreseeable operating conditions. Therefore, domain-aware DL testing methods become crucial to validate the properties and requirements for a DL-based system model beyond conventional performance testing. Crafting such testing methods needs an understanding of the simulated system design and a creative approach to incorporate the domain expertise without compromising the data-driven nature of DL and its acquired advantages.&lt;/p&gt;
&lt;p&gt;Therefore, we present our physics-guided adversarial testing method that leverages foreknown physics-grounded sensitivity rules associated with the system and metaheuristic search algorithms in order to expose regions of input space where the DL model violates the system properties distilled from the physics domain knowledge. We also demonstrate how the revealed adversarial inputs can be relevant for model regularization to penalize these physics inconsistencies and to improve the model reliability. Furthermore, the verification of DL model sensitivity at different data points against physics-grounded rules pose scalability challenges when it comes to sequential models that predict dynamical system behavior over successive time steps. In response to that, we encode the directional expected variation of the predicted quantities using a simple physics model that is stochastically calibrated on the experimental dataset. Thus, in addition to assessing the error rate of the sequential DL model on genuine flight tests, we examine the extent to which the trajectory of its time series predictions matches the expected dynamics of variation under craftly-designed synthetic flight scenarios.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Coding Session on Debugging Neural Networks Programs</title>
      <link>https://hoss-bb.github.io/talk/hands-on-coding-session-on-debugging-neural-networks-programs/</link>
      <pubDate>Tue, 03 Mar 2020 15:00:00 +0000</pubDate>
      <guid>https://hoss-bb.github.io/talk/hands-on-coding-session-on-debugging-neural-networks-programs/</guid>
      <description>&lt;p&gt;The goal of this hands-on session is to practice troubleshooting neural network training programs. The provided example consists of a VGG-like ConvNet trained on CIFAR-10 classification. If we fix the injected bugs (that I know), it should achieve more than 80% accuracy on the test set after more than 20 epochs. The debugging process is more relevant than finding the bug. Hence, we will walk through the steps you should follow towards hunting bugs. It is not about doing an in-depth review of code based on the participant&amp;rsquo;s Tensorflow coding skills, but rather about codifying verification routines in order to monitor the dynamics of critical variables and components that may indicate coding errors, misconfigurations, or training anomalies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hands-on Coding Session on Metamorphic Testing of Neural Networks</title>
      <link>https://hoss-bb.github.io/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/</link>
      <pubDate>Thu, 23 May 2019 14:00:00 +0000</pubDate>
      <guid>https://hoss-bb.github.io/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/</guid>
      <description>&lt;p&gt;Automating large-scale test case generation requires using multiple image transformations to build a large number of metamorphic transformations and their follow-up tests, in order to find DNN&amp;rsquo;s erroneous behaviors. In fact, the metamorphic transformation should be designed so that the transformed and original inputs are semantically equivalent. Thus, the first coding task is to implement metamorphic transformations on the input, assembling all the provided image-based transformations to guarantee the diversity of generated inputs. In order to estimate the level of logic explored by inputs, neuronal coverage criteria are employed, which estimate the amount of activation states covered by inputs. It is therefore essential that we store each valid synthetic input that fools the DNN under test. Hence, the second coding task is to develop the follow-up test that takes the logits returned by the DNN for the transformed data to check if the test fails or succeeds, then stores the synthetic images corresponding to the failed tests.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
