
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an MLOps specialist at Sycodal, a company developing collaborative robots for small to medium-sized industries. In this role, I work on maturing the ML engineering and deployment workflows for the vision recognition models, data-driven control modules, and predictive maintenance systems integrated into Sycodal’s robot fleet and IoT platform. I also supervise and support academic research projects in SE4ML.\nI hold a M.sc. and Ph.D. in software engineering from Polytechnique Montreal, where both of my dissertations were awarded the prize for best thesis in computer science. This involves being a resident ML researcher at Bombardier for two years, working on model testing and out-of-distribution detection approaches for tackling reliability and certification challenges of ML models in aircraft engineering problems. I published research papers on assurance quality of machine learning systems published in top-tier scientific journals and prestigious conferences. I participated in designing and developing open-source debugging and testing tools for deep learning programs. I am passionate about staying up-to-date on the latest MLOps initiatives, and I occasionally share my thoughts in technical workshops and blog posts.\n","date":1664928000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1664928000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an MLOps specialist at Sycodal, a company developing collaborative robots for small to medium-sized industries. In this role, I work on maturing the ML engineering and deployment workflows for the vision recognition models, data-driven control modules, and predictive maintenance systems integrated into Sycodal’s robot fleet and IoT platform.","tags":null,"title":"Houssem Ben Braiek","type":"authors"},{"authors":["Houssem Ben Braiek"],"categories":["Computer Vision"],"content":"","date":1664928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664928000,"objectID":"dfd63c0d018f5eec9ae91c37d0c3d48c","permalink":"","publishdate":"2022-10-05T00:00:00Z","relpermalink":"","section":"post","summary":"Modular, Multipath, Factorized, Compressed, Scalable… All in CNN Nowadays, but What Led us Here…We’ll take you through the major milestones…","tags":null,"title":"From VGGNet to EfficientNet: Key Milestones in the Evolution of CNN Design","type":"post"},{"authors":["Houssem Ben Braiek","Ali Tfaily","Foutse Khomh","Thomas Reid","Ciro Guida"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"4197362511b66ff9314ad810336d0ec2","permalink":"https://hoss-bb.github.io/publication/smood/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/smood/","section":"publication","summary":"Aircraft industry is constantly striving for more efficient design optimization methods in terms of human efforts, computation time, and resource consumption. Hybrid surrogate optimization maintains high results quality while providing rapid design assessments when both the surrogate model and the switch mechanism for eventually transitioning to the HF model are calibrated properly. Feedforward neural networks (FNNs) can capture highly nonlinear input-output mappings, yielding efficient surrogates for aircraft performance factors. However, FNNs often fail to generalize over the out-of-distribution (OOD) samples, which hinders their adoption in critical aircraft design optimization. Through SmOOD, our smoothness-based out-of-distribution detection approach, we propose to codesign a model-dependent OOD indicator with the optimized FNN surrogate, to produce a trustworthy surrogate model with selective but credible predictions. Unlike conventional uncertainty-grounded methods, SmOOD exploits inherent smoothness properties of the HF simulations to effectively expose OODs through revealing their suspicious sensitivities, thereby avoiding over-confident uncertainty estimates on OOD samples. By using SmOOD, only high-risk OOD inputs are forwarded to the HF model for re-evaluation, leading to more accurate results at a low overhead cost. Three aircraft performance models are investigated. Results show that FNN-based surrogates outperform their Gaussian Process counterparts in terms of predictive performance. Moreover, SmOOD does cover averagely 85% of actual OODs on all the study cases. When SmOOD plus FNN surrogates are deployed in hybrid surrogate optimization settings, they result in a decrease error rate of 34.65% and a computational speed up rate of 58.36 times, respectively.","tags":[],"title":"SmOOD: Smoothness-based Out-of-Distribution Detection Approach for Surrogate Neural Networks in Aircraft Design","type":"publication"},{"authors":["Houssem Ben Braiek","Thomas Reid","Foutse Khomh"],"categories":null,"content":"","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"1ed192aa6ca244f8270bceb5fcceef27","permalink":"https://hoss-bb.github.io/publication/physical/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/physical/","section":"publication","summary":"In the context of aircraft system performance assessment, deep learning technologies allow us to quickly infer models from experimental measurements, with less detailed system knowledge than usually required by physics-based modeling. However, this inexpensive model development also comes with new challenges regarding model trustworthiness. This article presents a novel approach, physics-guided adversarial machine learning (ML), which improves the confidence over the physics consistency of the model. The approach performs, first, a physics-guided adversarial testing phase to search for test inputs revealing behavioral system inconsistencies, while still falling within the range of foreseeable operational conditions. Then, it proceeds with a physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously uncovered counterexamples. Empirical evaluation on two aircraft system performance models shows the effectiveness of our adversarial ML approach in exposing physical inconsistencies of both the models and in improving their propensity to be consistent with physics domain knowledge.","tags":[],"title":"Physics-Guided Adversarial Machine Learning for Aircraft Systems Simulation","type":"publication"},{"authors":["Houssem Ben Braiek","Houssem Ben Braiek","Foutse Khomh","Sonia Bouzidi","Rania Zaatour"],"categories":null,"content":"","date":1658966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658966400,"objectID":"6fb502b10d76a605e0608eeead7021e5","permalink":"https://hoss-bb.github.io/publication/diverget/","publishdate":"2022-07-28T00:00:00Z","relpermalink":"/publication/diverget/","section":"publication","summary":"Quantization is one of the most applied Deep Neural Network (DNN) compression strategies, when deploying a trained DNN model on an embedded system or a cell phone. This is owing to its simplicity and adaptability to a wide range of applications and circumstances, as opposed to specific Artificial Intelligence (AI) accelerators and compilers that are often designed only for certain specific hardware (e.g., Google Coral Edge TPU). With the growing demand for quantization, ensuring the reliability of this strategy is becoming a critical challenge. Traditional testing methods, which gather more and more genuine data for better assessment, are often not practical because of the large size of the input space and the high similarity between the original DNN and its quantized counterpart. As a result, advanced assessment strategies have become of paramount importance. In this paper, we present DiverGet, a search-based testing framework for quantization assessment. DiverGet defines a space of metamorphic relations that simulate naturally-occurring distortions on the inputs. Then, it optimally explores these relations to reveal the disagreements among DNNs of different arithmetic precision. We evaluate the performance of DiverGet on state-of-the-art DNNs applied to hyperspectral remote sensing images. We chose the remote sensing DNNs as they're being increasingly deployed at the edge (e.g., high-lift drones) in critical domains like climate change research and astronomy. Our results show that DiverGet successfully challenges the robustness of established quantization techniques against naturally-occurring shifted data, and outperforms its most recent concurrent, DiffChaser, with a success rate that is (on average) four times higher.","tags":[],"title":"DiverGet: A Search-Based Software Testing Approach for Deep Neural Network Quantization Assessment","type":"publication"},{"authors":[],"categories":null,"content":"Data-driven AI (e.g., deep learning) has become a driving force and has been applied in many applications across diverse domains. The human-competitive performance makes them stand as core components in complicated software systems for tasks, e.g., computer vision (CV) and natural language processing (NLP). Corresponding to the increasing popularity of deploying more powerful and complicated DL models, there is also a pressing need to ensure the quality and reliability of these AI systems. However, the data-driven paradigm and black-box nature make such AI software fundamentally different from classical software. To this end, new software quality assurance techniques for AI-driven systems are thus challenging and needed. In this tutorial, we introduce the recent progress in AI Quality Assurance, especially for testing techniques for DNNs and provide hands-on experience. We will first give the details and discuss the difference between testing for traditional software and AI software. Then, we will provide hands-on tutorials on testing techniques for feed-forward neural networks (FNNs) with a CV use case and recurrent neural networks (RNNs) with an NLP use case. Finally, we will discuss with the audience the success and failures in achieving the full potential of testing AI software as well as possible improvements and research directions.\n","date":1652884200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652884200,"objectID":"143ba5764d208faf04567b5cba319834","permalink":"https://hoss-bb.github.io/talk/a-handon-guide-on-testing-deep-neural-networks/","publishdate":"2022-05-18T14:30:00Z","relpermalink":"/talk/a-handon-guide-on-testing-deep-neural-networks/","section":"event","summary":"(Part 1) Difference between traditional software and AI mode; (Part 2) Feedforward Neural Network Testing; (Part 3) Stateful Neural Network Analysis and Testing","tags":[],"title":"A Hand‑on Guide on Testing Deep Neural Networks","type":"event"},{"authors":[],"categories":null,"content":"With deep learning (DL) modeling, aircraft performance system simulators can be derived statistically without the extensive system knowledge required by physics-based modeling. Yet, DL’s rapid and economical simulations face serious trustworthiness challenges. Due to the high cost of aircraft flight tests, it is difficult to preserve held-out test datasets that cover the full range of the flight envelope for estimating the prediction errors of the learned DL model. Considering this risk of test data underrepresentation, even low-error models encounter credibility concerns when simulating and analyzing system behavior under all foreseeable operating conditions. Therefore, domain-aware DL testing methods become crucial to validate the properties and requirements for a DL-based system model beyond conventional performance testing. Crafting such testing methods needs an understanding of the simulated system design and a creative approach to incorporate the domain expertise without compromising the data-driven nature of DL and its acquired advantages.\nTherefore, we present our physics-guided adversarial testing method that leverages foreknown physics-grounded sensitivity rules associated with the system and metaheuristic search algorithms in order to expose regions of input space where the DL model violates the system properties distilled from the physics domain knowledge. We also demonstrate how the revealed adversarial inputs can be relevant for model regularization to penalize these physics inconsistencies and to improve the model reliability. Furthermore, the verification of DL model sensitivity at different data points against physics-grounded rules pose scalability challenges when it comes to sequential models that predict dynamical system behavior over successive time steps. In response to that, we encode the directional expected variation of the predicted quantities using a simple physics model that is stochastically calibrated on the experimental dataset. Thus, in addition to assessing the error rate of the sequential DL model on genuine flight tests, we examine the extent to which the trajectory of its time series predictions matches the expected dynamics of variation under craftly-designed synthetic flight scenarios.\n","date":1652884200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652884200,"objectID":"878b8b8311cabc57e7649d353cdfdb15","permalink":"https://hoss-bb.github.io/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/","publishdate":"2022-05-18T14:30:00Z","relpermalink":"/talk/domain-aware-deep-learning-testing-for-aircraft-system-models/","section":"event","summary":"How to leverage physics domain knowledge in testing the DL-enabled Aircraft System Models.","tags":[],"title":"Domain-Aware Deep Learning Testing for Aircraft System Models","type":"event"},{"authors":["Houssem Ben Braiek","Foutse Khomh"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"9040e46f7d6d92730ec840f9f71fbd1a","permalink":"https://hoss-bb.github.io/publication/deepchecker/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/deepchecker/","section":"publication","summary":"Nowadays, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters in order to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error-prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this paper, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design, TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs. We assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare it with Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker's on-execution validation of DNN-based program's properties succeeds in revealing several coding bugs and system misconfigurations, early on and at a low cost. Moreover, TheDeepChecker outperforms the SMD's offline rules verification on training logs in terms of detection accuracy and DL bugs coverage.","tags":[],"title":"Testing Feedforward Neural Networks Training Programs","type":"publication"},{"authors":["Houssem Ben Braiek"],"categories":["Maching Learning"],"content":"Many ML posts have discussed the complexities of classification problems, such as their class sensitivity and imbalanced labels. Even though regression problems are equally prevalent, there is no common preliminary analytics to assess the complexity of a given regression problem. Fortunately, ML researchers [1, 2] have adapted well-known classification complexity measures to numerically quantify the inherent difficulty of fitting regression datasets based on either Feature Correlation, Linearity, or Smoothness Measures.\nI will describe systematically the intuition, the definition, and the properties of each complexity measure, as well as how I implemented them in Python. This post is intended to empower the ML practitioners by handy means that help them distinguish between simple linear problems and more complex variations. As a matter of readability, I occasionally use functions from a helper module that I share on GitHub.\nFeature Correlation Measures (C) C1/C2: Maximum/Average Feature Correlation Intuition: The absolute value of correlation between a feature and a target reflects the strength of their relationship, which indicates how much information (scale from 0 to 1) the feature can provide on the target.\nDefinition: The C1 and C1 metrics represent, respectively, the maximum and the average of the feature correlation to the target output. The feature correlation is estimated by the absolute value of Spearman correlation [3], which is a non-parametric measure.\nProperties: C1 and C2 are within the range of [0, 1] and their high values suggest simpler regression problems.\nC3: Individual Feature Efficiency Intuition: Different subjects or samples were included in the data, so the relationship between a feature and a target could be strong in some cases and weak in others. The regression is simplified when features are highly correlated with the target in the majority of cases.\nDefinition: The C3 measure estimates the minimum proportion of examples that should be removed in order to produce a sub-dataset, where one feature is highly-correlated to the target output. Hence, it is necessary to fix a correlation threshold to determine if a feature can be regarded as highly-correlated with the output. For each feature, one can infer, iteratively, the proportion of examples that must be removed until the correlation to the output exceeds the predefined threshold. Thus, C3 consists of the minimum value of all the computed proportions.\nProperties: C3 yields values within [0, 1] and simpler regression problems would have low values.\nC4: Collective Feature Efficiency Intuition: Previously, we looked at the feature highly-correlated to the target when less rows should be removed. Here we consider the features’ contribution to explain the target variance collectively.\nDefinition: The C4 measure consists of the proportion of remaining examples that can be explained by any of the features. As a first step, one iterates over the features based on their correlation to the output, starting from the high values. Then, all examples whose outputs can be explained by a linear model [4] using the selected feature are eliminated. Indeed, the ability to explain the output of an example is determined by a residual value that is less than a predefined threshold. Therefore, the iterative process ends when all features have been analyzed or no examples are left, and the C4 measure is equal to the ratio of the remaining examples.\nProperties: C4 returns values within [0, 1] and regression problems with lower C4 values are simpler.\nLinearity Measures (L) L1/L2: Average/Absolute Error of Linear Model Intuition: If a linear function can approximately fit the data, the regression problem can be seen as simple involving linearly-distributed data points.\nDefinition: L1 or L2 measures represent, respectively, the sums of either the absolute residuals [5] or the squared residuals [5] from a multivariate linear regression model.\nProperties: The L1 and L2 are positive metrics with lower scores for simple regression problems that can be nearly solved with a linear function.\nL3: Non-linearity of a Linear Model Intuition: If the original data points are linearly distributed, a fit linear model can predict their interpolated variants within the distribution.\nDefinition: L3 measures how sensitive a linear regressor is to linearly-derived synthetic data. Random interpolations within the original data distribution can be used to generate the synthetic data. A pair of data points, x_iand x_jwith close outputs (low |y_i-y_j|), yield the new test input : (x_k,y_k)=(alpha*x_i + (1 — alpha)x_j, alphay_i + (1 — alpha)*y_j) , wherealphais a real number randomly sampled within [0, 1]. Using a linear model trained on the original data, the outputs of the derived random inputs are predicted. As a result, L3 is equal to the resulting mean square error over all synthetic inputs.\nProperties: L3 is always positive, and remains low as long as the regression problem …","date":1643587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643587200,"objectID":"edc14ed3b1f41c693fa8be97a33a087a","permalink":"","publishdate":"2022-01-31T00:00:00Z","relpermalink":"","section":"post","summary":"Half of the solution is comprehending your problem, but half of the comprehension is knowing how complicated it is.","tags":null,"title":"Are You Aware How Difficult Your Regression Problem Is?","type":"post"},{"authors":["Houssem Ben Braiek"],"categories":["Machine Learning"],"content":"The democratization of Machine Learning is driven by major tech firms and prestigious AI labs that have released powerful and ready-to-use ML libraries. With so many possible ML models available in this democratic ML age, it opens up the perennial question of ‘Which ML model to choose for a new statistical learning problem?’. But lucky you! In this post, I would discuss five simple and effective criteria that you can use to narrow down significantly your choices of ML models and avoid testing unsuitable models.\nAt the beginning, I would like to define two classes of ML models which we refer to in this post: (i) Parametric models, which summarize data with a finite set of parameters independent of the number of training data points. In other words, they try to fit the data into known-form mapping functions. For instance, a linear regression model assumes that the labeled data, D=(X,y), could be mapped by a linear function, y=f(X) , where f(X)=W.X+b; (ii) Non-parametric models, which make no assumptions about the form of the function mapping the data. For example, KNN attempts to predict the outcome for an unseen data point by combining the labels of its ‘k’ nearest data points. You can read more about both classes in [1].\nCriteria #1: Look at your data structure Data structure is the first criterion for selecting ML models. In fact, raw data, such as images, audio files, and pure text, are considered unstructured inputs. These types of data have been successfully processed by deep neural networks (DNN). I plan to address the issue of choosing the DNN architecture in a separate blog post. For now, I would recommend starting out with CNN[2], RNN[3], and Transformers[4] in the case of images, audio, and text. You may find the TDS posts [2, 3, 4] helpful to get started with these DNN architectures. Alternatively, tabular information that is derived from conventional databases and excel sheets is known as structured data. Hence, the next criteria would be the selection of ML models for structured data.\nCriteria #2: Measure the size of your data The second criterion relies on the size of data you have available.\nIn fact, there are ML models that are adapted to large amounts of data. In Scikit-learn, SGDClassifier and SGDRegressor represent the estimators that use SGD to find the best parameters of the selected parametric model. SGD stands for Stochastic Gradient Descent. It is a variant of gradient descent that estimates the gradient on a subset of data points (called a batch of data) at every iteration instead of using the full dataset. Hence, this gives birth to a new term, epoch, which means the number of iterations needed for the SGD to make a single pass over all the dataset. For a more detailed explanation of SGD, we refer to this post [5] on TDS. In case the data contains nonlinearities, it becomes necessary to implement feedforward neural networks using the Keras or Pytorch libraries. To avoid overfitting your data, it is crucial to design neural networks with a depth that is appropriate for the complexity of the problem, rather than using deep ones from the start.\nIn contrast, other ML models are not efficient at all when dealing with large training datasets. An example of this is the KNN algorithm. Since it retains the entire training dataset in memory, KNN would be unable to manage large numbers of instances and inference would be extremely slow due to the compute-intensive distances to estimate for each prediction. As a rule of thumb, we can use 100k data points, as stated in Scikit-learn official docs, in order to distinguish between large and small datasets.\nCriteria #3: Test the normality of your data Normality of the data means the tendency of data towards being normally distributed. I refer to these posts, [6] and [7], that introduce, respectively the normal distribution and conventional normality tests. Here, we are interested in separating the set of ML models with respect to their assumptions on data normality. Indeed, linear models including linear regressors and logistic regression classifiers, explicitly assume that the input data satisfy a bivariate or multivariate normal distribution. On the other side, decision tree and its popular ensemble-based derivative, random forest, work very well in the case of non-normally distributed data. For a broader scope of ML models, it is important to know that the parametric models actually work very well on data that follows foreknown distributions. Otherwise, we might, then, leverage non-parametric alternatives. Normality, as a common situation, is related to gaussian distribution trends in the data.\nCriteria #4: Examine the expected variance of your features’ importance Regarding normally-distributed data, we should consider further the expected variance of features’ importance to determine which parametric model to use. In fact, ML models with L1 penalization[8] on their coefficients will tend to eliminate the irrelevant features and converge to a …","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"1543e8c200f70d7a94e38c6069318f65","permalink":"","publishdate":"2022-01-05T00:00:00Z","relpermalink":"","section":"post","summary":"‘Which ML model to choose?’ is a perennial question, but believe me, this is one of the simplest yet very practical answer to it","tags":null,"title":"How to Make Systematic Choices of Machine Learning Models","type":"post"},{"authors":["Houssem Ben Braiek"],"categories":["SE4ML"],"content":"Python is a high-level programming language that is closer to natural human language. The benefit of this is that Python code is easy to comprehend and simple to implement. 48% of developers work with it, according to the 2021 Stack Overflow survey. In the coming years, this ratio will surely continue to grow.\nIndeed, Python has initially attracted academics and professionals who use coding as a way to prototype their ideas, to demonstrate concepts, and to implement proof-of-concepts. Nowadays, Python has already conquered many fields of software applications including Devops scripting, Machine Learning ( NumPy, pandas, sklearn, and Tensorflow and Web backends (Django and Flask).\nIn this post, I introduce a practical strategy to crack Python errors for beginners, starting by showing the essential strategy steps for this well-known exception, ZeroDivision, raised in the below code example.\nPython Error Cracking Steps on an Illustrative Example Our proposed error cracking strategy consists of three steps: (1) Scroll until you reach the bottom to find the error type; (2) Examine carefully the inline message that appears next to the error type in order to understand how it occurred; (3) Look closely at the trace of function calls and follow the horizontal arrows until you identify which line of code is faulty.\nThe ZeroDivisionError is a prevalent exception that points out directly to the root cause. We can consider them as specific exceptions such as FileNotFoundError and UnicodeError. Even beginners can find the code recipes on StackOverflow(SO) for fixing them. Nevertheless, there are generic exceptions, mostly the result of typos and careless mistakes during a deadline rush. Especially when a lot of the details in the error message pertain to your application code, googling these errors can be misleading due to the wide variability of possible causes.\nThe remaining paragraphs will provide you with an overview and explanation of some common types of errors. The above-described cracking method can better solve these types of errors. In this way, you’ll be able to save a great deal of time and effort spent reading posts about other SO users’ errors. Besides, you will also become more proficient at debugging yourself through this process, which will teach you more about the Python language.\nSyntaxError Raised when the parser encounters a syntax error. — Python Official Docs\nA Python parser cannot recognize the structure or sequence of tokens in a running program, which leads to syntax errors. Most often, this is due to missing or unnecessary characters, including commas, parentheses, etc.\nPython Code Snippet of Raised SyntaxError Exceptions In this example of SyntaxError, the occurred errors’ type and message are clear indicators that the code statement is syntactically invalid. Nevertheless, the difficulty of fault localization differs between them.\nFirst Cell: We look at the traceback to determine the cause (Here, there is just a single line of code 1). A small vertical arrow points out the exact location of the syntax error on this faulty line. So, you’ve spotted it. Just get rid of the comma in-between the for loops. Second Cell: It turns out that the code line (11) is not the faulty one, and the SyntaxErroris due to the missing parentheses in line (9). The parser, however, incorrectly identifies this location since it expects a closing parenthese there (rather than a new code statement), but it didn’t find it. Therefore, we should analyze reversely the code by starting at the indicated location of the SyntaxError. By not following this bottom-up debugging technique, you may find yourself searching StackOverflow for syntax errors in contexts that have similar keywords, names, and libraries to your own.\nNameError Raised when a local or global name is not found. — Python Official Docs\nBasically, developers get this error when they try to use a non-existent variable or function.\nThis is often caused by a misspelling in the name, such as the above example, where we spelled incorrectly the name of the function from the previous code snippet. This can happen when the developer forgets to import or declare the variable/function in the first place, as well as when he fails to run the cell that contains the underlying declaration.\nIndexError Raised when a sequence subscript is out of range. — Python Official Docs\nSimply, developers face this error when they try to access a non-existent cell in a sequence such as list, array, or dataframe.\nAs can be seen in the provided code example, the message associated to IndexErrorwould include useful details to remove the fault. Generally, the message reports the requested index and the size of the axis to which the cell belongs.\nKeyError Raised when a mapping key is not found in the set of existing keys. — Python Official Docs\nPrimarily, developers encounter this error when they try to access a non-existent key in a dictionary or dataframe.\nIt seems simple, but we sometimes …","date":1640908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640908800,"objectID":"ff43956fd25c897e109eb41e115143cc","permalink":"","publishdate":"2021-12-31T00:00:00Z","relpermalink":"","section":"post","summary":"As you crack manually an exception, the developer you become is more important than what you fix.","tags":null,"title":"How to Crack Python Errors Like a Pro","type":"post"},{"authors":["Amin Nikanjam","Mohammad Mehdi Morovati","Foutse Khomh","Houssem Ben Braiek"],"categories":null,"content":"","date":1640649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640649600,"objectID":"28d91dbe95d51fba6c4f6eeaee6e5455","permalink":"https://hoss-bb.github.io/publication/ase2021/","publishdate":"2021-12-28T00:00:00Z","relpermalink":"/publication/ase2021/","section":"publication","summary":"Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning (DL) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint, a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5% and a precision of 100%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.","tags":[],"title":"Faults in Deep Reinforcement Learning Programs: a Taxonomy and a Detection Approach","type":"publication"},{"authors":["Houssem Ben Braiek"],"categories":["SE4ML"],"content":"Like many ML practitioners with an SE background, I’ve observed on several occasions that the development process for ML-enabled software differs profoundly from that for conventional apps. Upon reading the empirical study[1] recently published by a group of SE researchers, I came up with a clear explanation of my early impressions. Indeed, this fascinating research work has explored thoroughly how machine learning impacts software development practices such as design and testing, as well as characteristics of work like skill variety and job complexity.\nMix of qualitative and quantitative analyses at a large scale\nEmpirical Study Methodology To begin, I will summarize the above-illustrated methodology applied by Wan et al.[1] in order to derive the statistically-confirmed statements of differences between ML and non-ML software engineering practices.\nIn the first round of interviews, 14 senior software professionals with experience in both ML and non-ML development were contacted to assemble the list of candidate statements of differences. Following open card sorting, a total of 80 statements were retained for the next steps. During the second round, three focus groups discuss the retained list, reducing it to 31 statements to be included in the survey. In the last round of the survey, hundreds of participants from various backgrounds responded to each statement on a five-point Likert scale, ranging from totally disagree to strongly agree. This will enable the comparison of the Likert distributions obtained from different groups of respondents for each statement. In fact, Wan et al.[1] recruited 342 respondents across 26 countries on 4 continents, both from ML and non-ML populations, to increase the credibility of their inferences.\nBased on the original survey results, 11 significant differences were found between ML and non-ML development. In order to make this article lightweight and easy to read, I will comment only on 3+3 statistically-confirmed statements about the differences between ML and non-ML software when it comes to development practices and characteristics of work.\nDevelopment Practices: Software Requirements “Collecting requirements involve a large number of preliminary experiments.”\nTo define the software requirements, ML developers agree that many experiments are needed. This refers to the stage where they try out some machine learning algorithms on samples of data in order to get preliminary insights. While the type of the data and the learning task often lead to certain models, several architectures and hyperparameters remain open to experimental evaluation.\nDevelopment Practices: Software Design “Detailed design is time-consuming and conducted in an iterative way.”\nIn terms of software design, ML practitioners consider the detailed design of ML systems is often the result of time-consuming, intensively-iterative processes. According to the previous statement, the feedback provided by the on-going experiments on the data would continuously influence the design decisions of future improvements.\nDevelopment Practices: Software Testing “Collecting testing dataset is labor intensive.”\nLast but not least, ML developers agree that collecting test data requires a lot of effort. This can be explained by the fact that most of the available data would influence the design and development of the ML system, then, a subset of held-out data would be used for testing. Even so, ML developers often keep only a relatively small fraction of the initial dataset and a further collection is necessary to assess the ML system’s different behaviors. A recent Google researchers work[2] clearly describes the underspecification pipeline problem, which is the most common issue with insignificant test datasets. In a nutshell, DL practitioners come up with many models that yield equivalent performances against the test datasets but would behave differently under deployment settings. Thus, having reliable test datasets is a must-have to separate effective models from ineffective ones before releasing the ML software system.\nWork Characteristics: Skill Variety “Developing my software requires knowledge in math, information theory, and statistics.”\nIn order to implement and integrate machine learning algorithms into software systems, developers must understand the mathematics inherent in the ML models. The latter are statistical learning algorithms that employ stochastic and iterative optimization processes. Coursera has an interesting course provided by Imperial College London on the maths behind ML.\nWork Characteristics: Task Identity “It is easy to make an accurate plan for development tasks of my software.”\nML developers disagree about having an accurate plan for development tasks. The confusion comes from the fact that at a high level we know the phases are: collect and preprocess data, feature engineering, construct ML models, train them, tune their hyperparameters, compare their performances against held-out test …","date":1640217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640217600,"objectID":"43a449ae637257b1befaf3cc3c10b5b3","permalink":"","publishdate":"2021-12-23T00:00:00Z","relpermalink":"","section":"post","summary":"ML has been proven to be a game-changer for SE. As soon as it is integrated into a software project, developers must adapt to substantial changes in the required skills and practices.","tags":null,"title":"Machine Learning Is Actually Altering Software Eng. Practices","type":"post"},{"authors":["Amin Nikanjam","Houssem Ben Braiek","Mohammad Mehdi Morovati","Foutse Khomh"],"categories":null,"content":"","date":1632787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632787200,"objectID":"2d24f938e8ed4e86fba4f486fd4052ef","permalink":"https://hoss-bb.github.io/publication/neuralint/","publishdate":"2021-09-28T00:00:00Z","relpermalink":"/publication/neuralint/","section":"publication","summary":"Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning (DL) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint, a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5% and a precision of 100%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.","tags":[],"title":"Automatic Fault Detection for Deep Learning Programs Using Graph Transformations","type":"publication"},{"authors":["Ettore Merlo","Mira Marhaba","Foutse Khomh","Houssem Ben Braiek","Giuliano Antoniol"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"e05a498442b8318ee809991c2027f77b","permalink":"https://hoss-bb.github.io/publication/imlse2020/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/imlse2020/","section":"publication","summary":"Neural network test cases are meant to exercise different reasoning paths in an architecture and used to validate the prediction outcomes. In this paper, we introduce \"computational profiles\" as vectors of neuron activation levels. We investigate the distribution of computational profile likelihood of metamorphic test cases with respect to the likelihood distributions of training, test and error control cases. We estimate the non-parametric probability densities of neuron activation levels for each distinct output class. Probabilities are inferred using training cases only, without any additional knowledge about metamorphic test cases. Experiments are performed by training a network on the MNIST Fashion library of images and comparing prediction likelihoods with those obtained from error control-data and from metamorphic test cases. Experimental results show that the distributions of computational profile likelihood for training and test cases are somehow similar, while the distribution of the random-noise control-data is always remarkably lower than the observed one for the training and testing sets. In contrast, metamorphic test cases show a prediction likelihood that lies in an extended range with respect to training, tests, and random noise. Moreover, the presented approach allows the independent assessment of different training classes and experiments to show that some of the classes are more sensitive to misclassifying metamorphic test cases than other classes. In conclusion, metamorphic test cases represent very aggressive tests for neural network architectures. Furthermore, since metamorphic test cases force a network to misclassify those inputs whose likelihood is similar to that of training cases, they could also be considered as adversarial attacks that evade defenses based on computational profile likelihood evaluation.","tags":[],"title":"Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases","type":"publication"},{"authors":["Hadhemi Jebnoun","Houssem Ben Braiek","Mohammad Masudur Rahman","Foutse Khomh"],"categories":null,"content":"","date":1593302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593302400,"objectID":"26d9ccf1c5b0fd1c3e3292aded4784b3","permalink":"https://hoss-bb.github.io/publication/msr2020/","publishdate":"2020-06-28T00:00:00Z","relpermalink":"/publication/msr2020/","section":"publication","summary":"Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications.","tags":[],"title":"The Scent of Deep Learning Code: An Empirical Study","type":"publication"},{"authors":["Houssem Ben Braiek","Foutse Khomh"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"4cfb42ec4880d229b170da04c3da7674","permalink":"https://hoss-bb.github.io/publication/review/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/review/","section":"publication","summary":"Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs.","tags":[],"title":"On Testing Machine Learning Programs","type":"publication"},{"authors":[],"categories":null,"content":"The goal of this hands-on session is to practice troubleshooting neural network training programs. The provided example consists of a VGG-like ConvNet trained on CIFAR-10 classification. If we fix the injected bugs (that I know), it should achieve more than 80% accuracy on the test set after more than 20 epochs. The debugging process is more relevant than finding the bug. Hence, we will walk through the steps you should follow towards hunting bugs. It is not about doing an in-depth review of code based on the participant’s Tensorflow coding skills, but rather about codifying verification routines in order to monitor the dynamics of critical variables and components that may indicate coding errors, misconfigurations, or training anomalies.\n","date":1583247600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583247600,"objectID":"8610bea9d7f8cec1bb537bf7f1bdd950","permalink":"https://hoss-bb.github.io/talk/hands-on-coding-session-on-debugging-neural-networks-programs/","publishdate":"2020-03-03T15:00:00Z","relpermalink":"/talk/hands-on-coding-session-on-debugging-neural-networks-programs/","section":"event","summary":"A tutorial on debugging Tensorflow programs that allows participants from academia and industry to learn the basics of debugging neural network training programs.","tags":[],"title":"Hands-on Coding Session on Debugging Neural Networks Programs","type":"event"},{"authors":null,"categories":null,"content":"","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"ed5fb8feaa7ea3812e2571099c30473c","permalink":"https://hoss-bb.github.io/project/dagstuhl/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/project/dagstuhl/","section":"project","summary":"I'm grateful to be a part of brainstorming sessions with focus groups of distinguished scientists and experts. These sessions aim to provide insights into how complex artificial intelligence and machine learning systems are created, debugged, and tested.","tags":null,"title":"Dagstuhl Seminar 20091 on Software Engineering for AI-ML-based Systems","type":"project"},{"authors":null,"categories":null,"content":"","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"f0ca14433afb7cac8bd0a6f7550822b6","permalink":"https://hoss-bb.github.io/project/shonan/","publishdate":"2019-10-20T00:00:00Z","relpermalink":"/project/shonan/","section":"project","summary":"I am honored to be invited to present our research on testing and debugging AI-intensive software and to exchange ideas with internationally renowned researchers and professionals.","tags":null,"title":"Shonan Meeting NO.156 on Software Engineering for Machine Learning Applications","type":"project"},{"authors":["Houssem Ben Braiek","Foutse Khomh"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"cd2c85bcff882598f2d7c972416ca95e","permalink":"https://hoss-bb.github.io/publication/deepevolution/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/deepevolution/","section":"publication","summary":"The increasing inclusion of Deep Learning (DL) models in safety-critical systems such as autonomous vehicles have led to the development of multiple model-based DL testing techniques. One common denominator of these testing techniques is the automated generation of test cases, e.g., new inputs transformed from the original training data with the aim to optimize some test adequacy criteria. So far, the effectiveness of these approaches has been hindered by their reliance on random fuzzing or transformations that do not always produce test cases with a good diversity. To overcome these limitations, we propose, DeepEvolution, a novel search-based approach for testing DL models that relies on metaheuristics to ensure a maximum diversity in generated test cases. We assess the effectiveness of DeepEvolution in testing computer-vision DL models and found that it significantly increases the neuronal coverage of generated test cases. Moreover, using DeepEvolution, we could successfully find several corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz (a coverage-guided fuzzing tool developed at Google Brain) in detecting latent defects introduced during the quantization of the models. These results suggest that search-based approaches can help build effective testing tools for DL systems.","tags":[],"title":"DeepEvolution: A Search-Based Testing Approach for Deep Neural Networks","type":"publication"},{"authors":["Houssem Ben Braiek","Foutse Khomh"],"categories":null,"content":"","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563753600,"objectID":"70774bab65c6473d8b58b0c8c867a4aa","permalink":"https://hoss-bb.github.io/publication/tfcheck/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/tfcheck/","section":"publication","summary":"The increasing inclusion of Machine Learning (ML) models in safety-critical systems like autonomous cars have led to the development of multiple model-based ML testing techniques. One common denominator of these testing techniques is their assumption that training programs are adequate and bug-free. These techniques only focus on assessing the performance of the constructed model using manually labeled data or automatically generated data. However, their assumptions about the training program are not always true as training programs can contain inconsistencies and bugs. In this paper, we examine training issues in ML programs and propose a catalog of verification routines that can be used to detect the identified issues, automatically. We implemented the routines in a Tensorflow-based library named TFCheck. Using TFCheck, practitioners can detect the aforementioned issues automatically. To assess the effectiveness of TFCheck, we conducted a case study with real-world, mutants, and synthetic training programs. Results show that TFCheck can successfully detect training issues in ML code implementations.","tags":[],"title":"TFCheck : A TensorFlow Library for Detecting Training Issues in Neural Network Programs","type":"publication"},{"authors":[],"categories":null,"content":"Automating large-scale test case generation requires using multiple image transformations to build a large number of metamorphic transformations and their follow-up tests, in order to find DNN’s erroneous behaviors. In fact, the metamorphic transformation should be designed so that the transformed and original inputs are semantically equivalent. Thus, the first coding task is to implement metamorphic transformations on the input, assembling all the provided image-based transformations to guarantee the diversity of generated inputs. In order to estimate the level of logic explored by inputs, neuronal coverage criteria are employed, which estimate the amount of activation states covered by inputs. It is therefore essential that we store each valid synthetic input that fools the DNN under test. Hence, the second coding task is to develop the follow-up test that takes the logits returned by the DNN for the transformed data to check if the test fails or succeeds, then stores the synthetic images corresponding to the failed tests.\n","date":1558620000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558620000,"objectID":"1c86e9fc3b9717a5909740f9aec3323a","permalink":"https://hoss-bb.github.io/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/","publishdate":"2019-05-23T14:00:00Z","relpermalink":"/talk/hands-on-coding-session-on-metamorphic-testing-of-neural-networks/","section":"event","summary":"This tutorial introduces the automation of metamorphic test generation for deep neural networks to attendees from academia and industry.","tags":[],"title":"Hands-on Coding Session on Metamorphic Testing of Neural Networks","type":"event"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hoss-bb.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Houssem Ben Braiek","Foutse Khomh","Bram Adams"],"categories":null,"content":"","date":1527379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527379200,"objectID":"86f9e75aca17a6d83b4a8bc0e18f67f0","permalink":"https://hoss-bb.github.io/publication/msr2018/","publishdate":"2018-05-27T00:00:00Z","relpermalink":"/publication/msr2018/","section":"publication","summary":"Recent advances in computing technologies and the availability of huge volumes of data have sparked a new machine learning (ML) revolution, where almost every day a new headline touts the demise of human experts by ML models on some task. Open source software development is rumoured to play a significant role in this revolution, with both academics and large corporations such as Google and Microsoft releasing their ML frameworks under an open source license. This paper takes a step back to examine and understand the role of open source development in modern ML, by examining the growth of the open source ML ecosystem on GitHub, its actors, and the adoption of frameworks over time. By mining LinkedIn and Google Scholar profiles, we also examine driving factors behind this growth (paid vs. voluntary contributors), as well as the major players who promote its democratization (companies vs. communities), and the composition of ML development teams (engineers vs. scientists). According to the technology adoption lifecycle, we find that ML is in between the stages of early adoption and early majority. Furthermore, companies are the main drivers behind open source ML, while the majority of development teams are hybrid teams comprising both engineers and professional scientists. The latter correspond to scientists employed by a company, and by far represent the most active profiles in the development of ML applications, which reflects the importance of a scientific background for the development of ML frameworks to complement coding skills. The large influence of cloud computing companies on the development of open source ML frameworks raises the risk of vendor lock-in. These frameworks, while open source, could be optimized for specific commercial cloud offerings.","tags":[],"title":"The Open-Closed Principle of Modern Machine Learning Frameworks'","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"50a2a39ddb511a1863c6aa41fd973ccd","permalink":"https://hoss-bb.github.io/project/aimia/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/project/aimia/","section":"project","summary":"Volunteering my technical skills, I work on sentiment analysis and opinion extraction tools for non-profit organizations to analyze comments written by their members.","tags":null,"title":"Aimia - Data Philanthropy 2018","type":"project"},{"authors":["Sonia Bouzidi","Houssem Ben Braiek"],"categories":null,"content":"","date":1517097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517097600,"objectID":"ba6df868af5cc652b97720f0e3ea8b65","permalink":"https://hoss-bb.github.io/publication/jei2018/","publishdate":"2018-01-28T00:00:00Z","relpermalink":"/publication/jei2018/","section":"publication","summary":"Due to the high-dimensional data space generated by hyperspectral sensors together with the real-time requirements of several remote sensing applications, it is important to accelerate hyperspectral data analysis. For this purpose, we aim to improve the performance of an existing classification algorithm and reduce its execution time. The proposed algorithm is based on sparse representation and using extended multiattribute profiles as spectral–spatial features, and sparse unmixing by variable splitting and augmented Lagrangian as the optimization method. The speeding up is mainly achieved by exploiting the interdependencies among iterative calls and providing an appropriate memorization technique to reduce the extra cost by factorizing the algebraic computations. The experimental results on two HSI data sets prove that the optimized algorithm is really faster than the original one while retaining the same classification accuracy. This study shows how useful it is to adapt the implementation of the generic module in order to become more appropriate to the application and to minimize the extra costs as much as possible.","tags":[],"title":"Improved Algorithm for Hyperspectral Image Classification","type":"publication"}]